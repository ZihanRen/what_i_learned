{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Devision Tree Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Structure Problem\n",
    "\n",
    "## Problem Description\n",
    "\n",
    "Create a function that converts a list of examples and attributes into a nested decision tree structure.\n",
    "\n",
    "## Input Format\n",
    "\n",
    "### Examples List\n",
    "The input consists of a list of dictionaries, where each dictionary represents a training example with attribute-value pairs and a class label ('PlayTennis').\n",
    "\n",
    "```python\n",
    "examples = [\n",
    "    {'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'No'},\n",
    "    {'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Strong', 'PlayTennis': 'No'},\n",
    "    {'Outlook': 'Overcast', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'Yes'},\n",
    "    {'Outlook': 'Rain', 'Temperature': 'Mild', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'Yes'}\n",
    "]\n",
    "```\n",
    "\n",
    "### Attributes List\n",
    "A list of attribute names to consider for splitting the decision tree:\n",
    "\n",
    "```python\n",
    "attributes = ['Outlook', 'Temperature', 'Humidity', 'Wind']\n",
    "```\n",
    "\n",
    "## Output Format\n",
    "\n",
    "The output should be a nested dictionary representing the decision tree structure:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'Outlook': {\n",
    "        'Sunny': {\n",
    "            'Humidity': {\n",
    "                'High': 'No',\n",
    "                'Normal': 'Yes'\n",
    "            }\n",
    "        },\n",
    "        'Overcast': 'Yes',\n",
    "        'Rain': {\n",
    "            'Wind': {\n",
    "                'Weak': 'Yes',\n",
    "                'Strong': 'No'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "## Structure Explanation\n",
    "\n",
    "1. Root Node\n",
    "   - The top level of the dictionary represents the root attribute ('Outlook')\n",
    "\n",
    "2. Internal Nodes\n",
    "   - Each non-leaf node is an attribute used for decision making\n",
    "   - The keys at each level are possible values for that attribute\n",
    "   - Values can be either:\n",
    "     - Another nested dictionary (for further splitting)\n",
    "     - A final classification ('Yes' or 'No')\n",
    "\n",
    "3. Leaf Nodes\n",
    "   - Terminal nodes containing the final classification\n",
    "   - Always contain either 'Yes' or 'No' as values\n",
    "\n",
    "## Reasoning\n",
    "\n",
    "The decision tree structure is determined based on the following logic:\n",
    "\n",
    "1. 'Outlook' is chosen as the root node based on its ability to split the data effectively\n",
    "2. For 'Outlook = Overcast', all examples lead to 'Yes', so it becomes a leaf node\n",
    "3. For 'Outlook = Sunny', further splitting on 'Humidity' is needed\n",
    "4. For 'Outlook = Rain', further splitting on 'Wind' is required\n",
    "5. The tree structure captures all decision paths from the training examples\n",
    "\n",
    "This structure allows for efficient classification of new examples by following the decision paths from root to leaf nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def calculate_entropy(labels):\n",
    "    '''\n",
    "    expected input is a list of labels\n",
    "    '''\n",
    "    label_counts = Counter(labels)\n",
    "    total_count = len(labels)\n",
    "    # calculate entropy - \\sum p(x) log2(p(x))\n",
    "    entropy = -sum((count / total_count) * math.log2(count / total_count) for count in label_counts.values())\n",
    "    return entropy\n",
    "\n",
    "def calculate_information_gain(examples, attr, target_attr):\n",
    "    # attr: attribute to considering spliting\n",
    "    # target_attr: target attribute - play_tennis\n",
    "    # exmaples: list of examples\n",
    "    total_entropy = calculate_entropy([example[target_attr] for example in examples])\n",
    "    # get unique values of attributes that we are splitting:\n",
    "    # e.g. for outlook: ['sunny', 'overcast', 'rainy']\n",
    "    values = set(example[attr] for example in examples)\n",
    "    attr_entropy = 0\n",
    "    for value in values:\n",
    "        value_subset = [example[target_attr] for example in examples if example[attr] == value]\n",
    "        # this is like a loss function for the current attribute\n",
    "        # entropy of the subset\n",
    "        # if entropy is 0, then we have split that leads to pure class\n",
    "        value_entropy = calculate_entropy(value_subset)\n",
    "        # weightaed sum of entropy\n",
    "        # current attribute entropy = \\sum p(x) * entropy(x)\n",
    "        # this tells us that under current splitting, how much entropy is reduced\n",
    "        attr_entropy += (len(value_subset) / len(examples)) * value_entropy\n",
    "    # Information Gain = Total Entropy - Weighted Attribute Entropy\n",
    "    # IG = H(S) - Î£((|Sv|/|S|) * H(Sv))\n",
    "    return total_entropy - attr_entropy\n",
    "\n",
    "def majority_class(examples, target_attr):\n",
    "    return Counter([example[target_attr] for example in examples]).most_common(1)[0][0]\n",
    "\n",
    "def learn_decision_tree(examples, attributes, target_attr):\n",
    "    if not examples:\n",
    "        return 'No examples'\n",
    "    # check if all examples have the same class - no need to split further if True\n",
    "    if all(example[target_attr] == examples[0][target_attr] for example in examples):\n",
    "        return examples[0][target_attr]\n",
    "    # if no attributes left to split on, return majority class - majority voting\n",
    "    if not attributes:\n",
    "        return majority_class(examples, target_attr)\n",
    "    \n",
    "    # assign a information gain to each attribute\n",
    "    # th best splitting attribute is the one with the highest information gain\n",
    "    gains = {attr: calculate_information_gain(examples, attr, target_attr) for attr in attributes}\n",
    "\n",
    "    # attribute with maximum information gain\n",
    "    best_attr = max(gains, key=gains.get)\n",
    "\n",
    "    tree = {best_attr: {}}\n",
    "\n",
    "    unique_values = set(example[best_attr] for example in examples)\n",
    "\n",
    "    # unique values of the best attribute\n",
    "    # e.g. for outlook: ['sunny', 'overcast', 'rainy']\n",
    "    \n",
    "    for value in unique_values:\n",
    "        subset = [example for example in examples if example[best_attr] == value]\n",
    "        new_attributes = attributes.copy()\n",
    "\n",
    "        # since we have split on the best attribute, we remove it from the list of attributes\n",
    "        new_attributes.remove(best_attr)\n",
    "        \n",
    "        # recursively build the tree\n",
    "        # with new subset and attributes\n",
    "        subtree = learn_decision_tree(subset, new_attributes, target_attr)\n",
    "        tree[best_attr][value] = subtree\n",
    "    \n",
    "    return tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Outlook': {'Overcast': 'Yes', 'Sunny': 'No', 'Rain': 'Yes'}}\n"
     ]
    }
   ],
   "source": [
    "# Example data\n",
    "examples = [\n",
    "    {'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', \n",
    "     'Wind': 'Weak', 'PlayTennis': 'No'},\n",
    "    {'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', \n",
    "     'Wind': 'Strong', 'PlayTennis': 'No'},\n",
    "    {'Outlook': 'Overcast', 'Temperature': 'Hot', 'Humidity': 'High', \n",
    "     'Wind': 'Weak', 'PlayTennis': 'Yes'},\n",
    "    {'Outlook': 'Rain', 'Temperature': 'Mild', 'Humidity': 'High', \n",
    "     'Wind': 'Weak', 'PlayTennis': 'Yes'}\n",
    "]\n",
    "\n",
    "attributes = ['Outlook', 'Temperature', 'Humidity', 'Wind']\n",
    "target_attr = 'PlayTennis'\n",
    "\n",
    "\n",
    "\n",
    "# Build tree\n",
    "tree = learn_decision_tree(examples, attributes, target_attr)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_entropy([example[target_attr] for example in examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
