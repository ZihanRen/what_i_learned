{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss functions\n",
    "\n",
    "Code loss functions related with neural network for different tasks\n",
    "\n",
    "Some reference: https://neptune.ai/blog/pytorch-loss-functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Likelihood Estimation\n",
    "\n",
    "\n",
    "a good intro can be found [link](https://dasha.ai/en-us/blog/log-loss-function). In the multi-class context, the negative log loss is defined as\n",
    "\n",
    "$$\n",
    "\\text{Log Loss} = -\\frac{1}{q} \\sum_{i=1}^q \\sum_{j=1}^l y_{ij} \\log(a_{ij})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $q$ is the total number of samples\n",
    "- $l$ is the total number of classes\n",
    "- $a_{ij}$ is the probability assigned by the algorithm for the $i$-th sample belonging to the $j$-th class\n",
    "- $y_{ij}$ is 1 if the $i$-th sample belongs to the $j$-th class, and 0 otherwise\n",
    "\n",
    "This formulation is equivalent to multiplying the probability distribution with a one-hot encoded matrix, where:\n",
    "- Number of rows = Number of samples\n",
    "- Number of columns = Number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2629)\n",
      "1.262864351272583\n"
     ]
    }
   ],
   "source": [
    "# log likelihood estimation function\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# input prob should be 2*2\n",
    "input_prob = torch.tensor([[0.1, 0.9], [0.2, 0.8]])\n",
    "target = torch.tensor([0, 1])\n",
    "loss = nn.NLLLoss()\n",
    "output = loss(torch.log(input_prob), target)\n",
    "print(output)\n",
    "\n",
    "\n",
    "# calculate using numpy\n",
    "import numpy as np\n",
    "np_input_prob = input_prob.numpy()\n",
    "loss_np_sum = -np.log(np_input_prob[0, 0]) - np.log(np_input_prob[1, 1])\n",
    "print(loss_np_sum/2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss\n",
    "\n",
    "Cross Entropy loss is closely related to minimizing negative log likelihood. It's a fundamental concept in information theory and machine learning, particularly useful for classification tasks.\n",
    "\n",
    "The Cross Entropy between two probability distributions $p$ and $q$ is defined as:\n",
    "\n",
    "$$\n",
    "H(p,q) = -\\sum_x p(x) \\log q(x)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $p(x)$ is the ground truth probability distribution (typically a one-hot encoded representation of labels)\n",
    "- $q(x)$ is the predicted probability distribution from the model\n",
    "\n",
    "In the context of machine learning:\n",
    "- $p$ represents the true label distribution (often a one-hot vector for classification tasks)\n",
    "- $q$ represents the model's predicted probabilities\n",
    "\n",
    "Key points:\n",
    "1. Cross Entropy measures the dissimilarity between two probability distributions.\n",
    "2. Minimizing Cross Entropy is equivalent to maximizing the likelihood of the true labels under the model's predictions.\n",
    "3. for multi-class prediction, min cross entropy = max negative log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch CrossEntropyLoss: 0.8042942881584167\n",
      "NumPy Cross-Entropy: 0.80429435\n",
      "PyTorch NLLLoss with log_softmax: 0.8042942881584167\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch implementation\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Input should be raw scores (logits), not probabilities\n",
    "input_logits = torch.tensor([[0.1, 0.9], [0.2, 0.8]])\n",
    "target = torch.tensor([0, 1])\n",
    "output = loss_fn(input_logits, target)\n",
    "print(\"PyTorch CrossEntropyLoss:\", output.item())\n",
    "\n",
    "\n",
    "# NumPy implementation\n",
    "np_input_prob = torch.softmax(input_logits, dim=1).numpy()\n",
    "np_target = target.numpy()\n",
    "selected_probs = []\n",
    "\n",
    "# get one hot encoded array * probability\n",
    "for i in range(len(np_target)):\n",
    "    selected_probs.append(np_input_prob[i, np_target[i]])\n",
    "selected_probs = np.array(selected_probs)\n",
    "\n",
    "\n",
    "loss_np = -np.log(selected_probs).mean()\n",
    "print(\"NumPy Cross-Entropy:\", loss_np)\n",
    "\n",
    "\n",
    "# For comparison, let's also use NLLLoss with log probabilities\n",
    "log_probs = torch.log_softmax(input_logits, dim=1)\n",
    "nll_loss = nn.NLLLoss()\n",
    "nll_output = nll_loss(log_probs, target)\n",
    "print(\"PyTorch NLLLoss with log_softmax:\", nll_output.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3100255, 0.6456563], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_input_prob[np.arange(len(np_target)), np_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3100255, 0.6456563], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_input_prob[np.arange(len(np_target)), np_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dice score\n",
    "\n",
    "* This is especially helpful for image segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KL divergence\n",
    "\n",
    "* KL quantify how much one probability distribution differs from another probability distribution\n",
    "\n",
    "Applications:\n",
    "* loss func in VAE\n",
    "* InfoGAN\n",
    "\n",
    "\n",
    "$KL (p||q) = \\sum_x {  p(x) log(\\frac{p(x)}{q(x)})  }$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# KL ( p||Q )  =  \\sum { P(x)log(P(x))  /  Q(x) }\n",
    "\n",
    "events = ['red', 'green', 'blue']\n",
    "p = [0.10, 0.40, 0.50]\n",
    "q = [0.80, 0.15, 0.05]\n",
    "\n",
    "# calculate the kl divergence\n",
    "from math import log2\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "\treturn sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))\n",
    "\n",
    "\n",
    "a = sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
