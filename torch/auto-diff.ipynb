{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some simple exploration of automatic differentiation - by Torch\n",
    "\n",
    "Some references:\n",
    "* https://marksaroufim.medium.com/automatic-differentiation-step-by-step-24240f97a6e6\n",
    "* [PyTorch Tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#gradients)\n",
    "\n",
    "\n",
    "#### Example 1\n",
    "\n",
    "$y = f(x_1,x_2) = x_1x_2-sin(x_2)$ \\\n",
    "The goal is to compute:\\\n",
    "$f'(x_1=2,x_2=3)$\n",
    "\n",
    "Firstly, some hand calculations: considering each operation as node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final result is 5.858879991940133\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "x1 = 2\n",
    "x2 = 3\n",
    "\n",
    "# forward process\n",
    "node_1 = 2\n",
    "node_2 = 3\n",
    "node_3 = node_1*node_2\n",
    "node_4 = math.sin(node_2)\n",
    "node_5 = node_3 - node_4\n",
    "print(f'Final result is {node_5}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### backward differentiation - take derivative to x2\n",
    "\n",
    "Computatinoal graph - forward mode differtiation\n",
    "\n",
    "$\\frac{dx_1}{dx_2} = 0$\n",
    "$\\frac{dx_2}{dx_2} = 1$\\\n",
    "$\\frac{dn_3}{dx_2} = \\frac{dx_1*x_2}{dx_2}$ = $\\frac{dx_1}{dx_2}*x2 + \\frac{dx_2}{dx_2}*x_1$\\\n",
    "$\\frac{dn_4}{dx_2}$ = $cos(n_2)$\\\n",
    "$\\frac{dn_5}{dx_2}$ = $\\frac{dn_3}{x_2}-\\frac{dn_4}{x_2}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final derivative relative to x2 is 2.989992496600445\n",
      "node3 derivative is 2\n",
      "node4 derivative is -0.9899924966004454\n",
      "node2 derivative is 1\n",
      "node1 derivative is 0\n"
     ]
    }
   ],
   "source": [
    "dn1_x2 = 0 # dx1/x2 = 0\n",
    "dn2_x2 = 1 # dx2/x2 = 1\n",
    "dn3_x2 = 0*node_2 + 1*node_1 #d(x1*x2)/dx2 = dx1/dx2 * x2 + dx2/dx2*x1\\\n",
    "dn4_x2 = math.cos(node_2)\n",
    "dn5_x5 = dn3_x2 - dn4_x2\n",
    "print(f'Final derivative relative to x2 is {dn5_x5}')\n",
    "print(f'node3 derivative is {dn3_x2}')\n",
    "print(f'node4 derivative is {dn4_x2}')\n",
    "print(f'node2 derivative is {dn2_x2}')\n",
    "print(f'node1 derivative is {dn1_x2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reverse mode differtiation - chain rule\n",
    "\n",
    "$y = f(x_1,x_2) = x_1x_2-sin(x_2)$ \\\n",
    "The goal is to compute:\\\n",
    "$f'(x_1=2,x_2=3)$\n",
    "\n",
    "\n",
    "$dn_5$ = $\\frac{dn_5}{dn_5}$ = $1$\n",
    "\n",
    "$dn_4$ = $ dn_5 * \\frac{dn_5}{dn_4}$ = $-1$, remember negative sign before node 4\\\n",
    "$dn_3$ = $dn_5 * \\frac{dn_5}{dn_3}$ = $1$\\\n",
    "$dn_2$ = $dn_4 * \\frac{dn_4}{dn_2} +  dn_3 * \\frac{dn_3}{dn_2}$ = $dn_4 * cos(dn_2) + dn_3* \\frac{d (n_1 * n_2)}{dn_2} $ = -1* cos(3) + 1*2 = 1\\\n",
    "$dn_1$ = $dn_3 * \\frac{dn_3}{dn_1}$ = $1*n_2 = 3$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final children x2 derivative is 2.989992496600445\n",
      "Final Children x1 derivative is 3\n"
     ]
    }
   ],
   "source": [
    "# parent node\n",
    "d5 = 1\n",
    "d5_d4 = -1\n",
    "d5_d3 = 1\n",
    "# next parent node\n",
    "d4 = d5*d5_d4\n",
    "d3 = d5*d5_d3\n",
    "# children node - to d2 (x2)\n",
    "d4_d2 = d3*math.cos(node_2)\n",
    "d3_d2 = d3*node_1\n",
    "\n",
    "d2 = d4*d4_d2 + d3*d3_d2\n",
    "d3_d1 = node_2 * 1\n",
    "d1 = d3 * d3_d1\n",
    "print(f'Final children x2 derivative is {d2}')\n",
    "print(f'Final Children x1 derivative is {d1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on pytorch\n",
    "\n",
    "Automatic differentiations to different nodes: chain rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x1 = torch.FloatTensor([2])\n",
    "x1.requires_grad = True\n",
    "x2 = torch.FloatTensor([3])\n",
    "x2.requires_grad = True\n",
    "loss = x1*x2 - torch.sin(x2)\n",
    "# loss_mean = loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.9900])\n",
      "tensor([3.])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(x2.grad)\n",
    "print(x1.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retain Graph\n",
    "\n",
    "$y_1 = f(x_1,x_2) = x_1x_2-sin(x_2)$\\\n",
    "$y_2 = f(x_1,x_2) = x_1x_2+sin(x_2)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradident of x2 from loss 1 is tensor([2.9900])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.FloatTensor([2])\n",
    "x1.requires_grad = True\n",
    "x2 = torch.FloatTensor([3])\n",
    "x2.requires_grad = True\n",
    "loss1 = x1*x2 - torch.sin(x2)\n",
    "loss2 = x1*x2 + torch.sin(x2)\n",
    "loss1.backward()\n",
    "print(\"The gradident of x2 from loss 1 is {}\".format(x2.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradident of x2 from loss 2 is tensor([1.0100])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.FloatTensor([2])\n",
    "x1.requires_grad = True\n",
    "x2 = torch.FloatTensor([3])\n",
    "x2.requires_grad = True\n",
    "loss1 = x1*x2 - torch.sin(x2)\n",
    "loss2 = x1*x2 + torch.sin(x2)\n",
    "loss2.backward()\n",
    "print(\"The gradident of x2 from loss 2 is {}\".format(x2.grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradident of x2 from multiple backward accumulated is tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.FloatTensor([2])\n",
    "x1.requires_grad = True\n",
    "x2 = torch.FloatTensor([3])\n",
    "x2.requires_grad = True\n",
    "loss1 = x1*x2 - torch.sin(x2)\n",
    "loss2 = x1*x2 + torch.sin(x2)\n",
    "loss1.backward()\n",
    "loss2.backward()\n",
    "print(\"The gradident of x2 from multiple backward accumulated is {}\".format(x2.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradident of x2 from one backpropagation but with summation is tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.FloatTensor([2])\n",
    "x1.requires_grad = True\n",
    "x2 = torch.FloatTensor([3])\n",
    "x2.requires_grad = True\n",
    "loss1 = x1*x2 - torch.sin(x2)\n",
    "loss2 = x1*x2 + torch.sin(x2)\n",
    "loss = loss1 + loss2\n",
    "loss.backward()\n",
    "print(\"The gradident of x2 from one backpropagation but with summation is {}\".format(x2.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can backpropagate twice with retain_graph=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradident of x2 from one backpropagation but with summation is tensor([8.])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.FloatTensor([2])\n",
    "x1.requires_grad = True\n",
    "x2 = torch.FloatTensor([3])\n",
    "x2.requires_grad = True\n",
    "loss1 = x1*x2 - torch.sin(x2)\n",
    "loss2 = x1*x2 + torch.sin(x2)\n",
    "loss = loss1 + loss2\n",
    "loss.backward(retain_graph=True)\n",
    "loss.backward()\n",
    "print(\"The gradident of x2 from one backpropagation but with summation is {}\".format(x2.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradident of x2 from one backpropagation but with summation and weights is tensor([3.4950])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.FloatTensor([2])\n",
    "x1.requires_grad = True\n",
    "x2 = torch.FloatTensor([3])\n",
    "x2.requires_grad = True\n",
    "loss1 = x1*x2 - torch.sin(x2)\n",
    "loss2 = x1*x2 + torch.sin(x2)\n",
    "loss = loss1 + loss2*0.5\n",
    "loss.backward()\n",
    "print(\"The gradident of x2 from one backpropagation but with summation and weights is {}\".format(x2.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You have to retain graph = true if you backward twice since there are shared intermediate variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.])\n",
      "tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.FloatTensor([2])\n",
    "x1.requires_grad = True\n",
    "x2 = torch.FloatTensor([3])\n",
    "x2.requires_grad = True\n",
    "\n",
    "# y is an intermediate variable.\n",
    "y = x1 * x2  \n",
    "\n",
    "# Two losses that depend on the intermediate variable y.\n",
    "loss1 = y - torch.sin(x2)\n",
    "loss2 = y + torch.sin(x2)\n",
    "\n",
    "# Backpropagating on loss1 requires us to retain the graph if we want to backpropagate on loss2 afterwards,\n",
    "# because loss2 depends on the same y that was created before.\n",
    "loss1.backward(retain_graph=True)  # retain_graph=True is needed if we want to backpropagate through y again.\n",
    "loss2.backward()  # Now it's fine without retain_graph=True because this is the last time we backpropagate.\n",
    "\n",
    "print(x1.grad)  # The gradient of x1 has accumulated from both backward passes.\n",
    "print(x2.grad)  # The gradient of x2 has also accumul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shared intermediate variables version :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/journel/s0/zur74/what_i_learned/torch/auto-diff.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bjournel.eme.psu.edu/journel/s0/zur74/what_i_learned/torch/auto-diff.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Backpropagating on loss1 requires us to retain the graph if we want to backpropagate on loss2 afterwards,\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bjournel.eme.psu.edu/journel/s0/zur74/what_i_learned/torch/auto-diff.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# because loss2 depends on the same y that was created before.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bjournel.eme.psu.edu/journel/s0/zur74/what_i_learned/torch/auto-diff.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m loss1\u001b[39m.\u001b[39mbackward()  \u001b[39m# retain_graph=True is needed if we want to backpropagate through y again.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bjournel.eme.psu.edu/journel/s0/zur74/what_i_learned/torch/auto-diff.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m loss2\u001b[39m.\u001b[39;49mbackward()  \u001b[39m# Now it's fine without retain_graph=True because this is the last time we backpropagate.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bjournel.eme.psu.edu/journel/s0/zur74/what_i_learned/torch/auto-diff.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(x1\u001b[39m.\u001b[39mgrad)  \u001b[39m# The gradient of x1 has accumulated from both backward passes.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bjournel.eme.psu.edu/journel/s0/zur74/what_i_learned/torch/auto-diff.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(x2\u001b[39m.\u001b[39mgrad)  \u001b[39m# The gradient of x2 has also accumul\u001b[39;00m\n",
      "File \u001b[0;32m/journel/s0/zur74/venv/pnm-old/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m/journel/s0/zur74/venv/pnm-old/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "x1 = torch.FloatTensor([2])\n",
    "x1.requires_grad = True\n",
    "x2 = torch.FloatTensor([3])\n",
    "x2.requires_grad = True\n",
    "\n",
    "# y is an intermediate variable.\n",
    "y = x1 * x2  \n",
    "\n",
    "# Two losses that depend on the intermediate variable y.\n",
    "loss1 = y - torch.sin(x2)\n",
    "loss2 = y + torch.sin(x2)\n",
    "\n",
    "# Backpropagating on loss1 requires us to retain the graph if we want to backpropagate on loss2 afterwards,\n",
    "# because loss2 depends on the same y that was created before.\n",
    "loss1.backward()  # retain_graph=True is needed if we want to backpropagate through y again.\n",
    "loss2.backward()  # Now it's fine without retain_graph=True because this is the last time we backpropagate.\n",
    "\n",
    "print(x1.grad)  # The gradient of x1 has accumulated from both backward passes.\n",
    "print(x2.grad)  # The gradient of x2 has also accumul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pnm-new')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "769950f749cf263e04e34dc3f2141dd64b68c6436fc98dadab253467171bd3e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
