{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some simple exploration of automatic differentiation - by Torch\n",
    "\n",
    "Some references:\n",
    "* https://marksaroufim.medium.com/automatic-differentiation-step-by-step-24240f97a6e6\n",
    "* [PyTorch Tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#gradients)\n",
    "\n",
    "\n",
    "#### Example 1\n",
    "\n",
    "$y = f(x_1,x_2) = x_1x_2-sin(x_2)$ \\\n",
    "The goal is to compute:\\\n",
    "$f'(x_1=2,x_2=3)$\n",
    "\n",
    "Firstly, some hand calculations: considering each operation as node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final result is 5.858879991940133\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "x1 = 2\n",
    "x2 = 3\n",
    "\n",
    "# forward process\n",
    "node_1 = 2\n",
    "node_2 = 3\n",
    "node_3 = node_1*node_2\n",
    "node_4 = math.sin(node_2)\n",
    "node_5 = node_3 - node_4\n",
    "print(f'Final result is {node_5}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### backward differentiation - take derivative to x2\n",
    "\n",
    "Computatinoal graph - forward mode differtiation\n",
    "\n",
    "$\\frac{dx_1}{dx_2} = 0$\n",
    "$\\frac{dx_2}{dx_2} = 1$\\\n",
    "$\\frac{dn_3}{dx_2} = \\frac{dx_1*x_2}{dx_2}$ = $\\frac{dx_1}{dx_2}*x2 + \\frac{dx_2}{dx_2}*x_1$\\\n",
    "$\\frac{dn_4}{dx_2}$ = $cos(n_2)$\\\n",
    "$\\frac{dn_5}{dx_2}$ = $\\frac{dn_3}{x_2}-\\frac{dn_4}{x_2}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final derivative relative to x2 is 2.989992496600445\n",
      "node3 derivative is 2\n",
      "node4 derivative is -0.9899924966004454\n",
      "node2 derivative is 1\n",
      "node1 derivative is 0\n"
     ]
    }
   ],
   "source": [
    "dn1_x2 = 0 # dx1/x2 = 0\n",
    "dn2_x2 = 1 # dx2/x2 = 1\n",
    "dn3_x2 = 0*node_2 + 1*node_1 #d(x1*x2)/dx2 = dx1/dx2 * x2 + dx2/dx2*x1\\\n",
    "dn4_x2 = math.cos(node_2)\n",
    "dn5_x5 = dn3_x2 - dn4_x2\n",
    "print(f'Final derivative relative to x2 is {dn5_x5}')\n",
    "print(f'node3 derivative is {dn3_x2}')\n",
    "print(f'node4 derivative is {dn4_x2}')\n",
    "print(f'node2 derivative is {dn2_x2}')\n",
    "print(f'node1 derivative is {dn1_x2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reverse mode differtiation - chain rule\n",
    "\n",
    "$y = f(x_1,x_2) = x_1x_2-sin(x_2)$ \\\n",
    "The goal is to compute:\\\n",
    "$f'(x_1=2,x_2=3)$\n",
    "\n",
    "\n",
    "$dn_5$ = $\\frac{dn_5}{dn_5}$ = $1$\n",
    "\n",
    "$dn_4$ = $ dn_5 * \\frac{dn_5}{dn_4}$ = $-1$, remember negative sign before node 4\\\n",
    "$dn_3$ = $dn_5 * \\frac{dn_5}{dn_3}$ = $1$\\\n",
    "$dn_2$ = $dn_4 * \\frac{dn_4}{dn_2} +  dn_3 * \\frac{dn_3}{dn_2}$ = $dn_4 * cos(dn_2) + dn_3* \\frac{d (n_1 * n_2)}{dn_2} $ = -1* cos(3) + 1*2 = 1\\\n",
    "$dn_1$ = $dn_3 * \\frac{dn_3}{dn_1}$ = $1*n_2 = 3$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final children x2 derivative is 2.989992496600445\n",
      "Final Children x1 derivative is 3\n"
     ]
    }
   ],
   "source": [
    "# parent node\n",
    "d5 = 1\n",
    "d5_d4 = -1\n",
    "d5_d3 = 1\n",
    "# next parent node\n",
    "d4 = d5*d5_d4\n",
    "d3 = d5*d5_d3\n",
    "# children node - to d2 (x2)\n",
    "d4_d2 = d3*math.cos(node_2)\n",
    "d3_d2 = d3*node_1\n",
    "\n",
    "d2 = d4*d4_d2 + d3*d3_d2\n",
    "d3_d1 = node_2 * 1\n",
    "d1 = d3 * d3_d1\n",
    "print(f'Final children x2 derivative is {d2}')\n",
    "print(f'Final Children x1 derivative is {d1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on pytorch\n",
    "\n",
    "Automatic differentiations to different nodes: chain rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x1 = torch.FloatTensor([2])\n",
    "x1.requires_grad = True\n",
    "x2 = torch.FloatTensor([3])\n",
    "x2.requires_grad = True\n",
    "loss = x1*x2 - torch.sin(x2)\n",
    "# loss_mean = loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.9900])\n",
      "tensor([3.])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(x2.grad)\n",
    "print(x1.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retain Graph\n",
    "\n",
    "$y_1 = f(x_1,x_2) = x_1x_2-sin(x_2)$\\\n",
    "$y_2 = f(x_1,x_2) = x_1x_2+sin(x_2)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradident of x2 from loss 1 is tensor([2.9900])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.FloatTensor([2])\n",
    "x1.requires_grad = True\n",
    "x2 = torch.FloatTensor([3])\n",
    "x2.requires_grad = True\n",
    "loss1 = x1*x2 - torch.sin(x2)\n",
    "loss2 = x1*x2 + torch.sin(x2)\n",
    "loss1.backward()\n",
    "print(\"The gradident of x2 from loss 1 is {}\".format(x2.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradident of x2 from loss 2 is tensor([1.0100])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.FloatTensor([2])\n",
    "x1.requires_grad = True\n",
    "x2 = torch.FloatTensor([3])\n",
    "x2.requires_grad = True\n",
    "loss1 = x1*x2 - torch.sin(x2)\n",
    "loss2 = x1*x2 + torch.sin(x2)\n",
    "loss2.backward()\n",
    "print(\"The gradident of x2 from loss 2 is {}\".format(x2.grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradident of x2 from multiple backward is tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.FloatTensor([2])\n",
    "x1.requires_grad = True\n",
    "x2 = torch.FloatTensor([3])\n",
    "x2.requires_grad = True\n",
    "loss1 = x1*x2 - torch.sin(x2)\n",
    "loss2 = x1*x2 + torch.sin(x2)\n",
    "loss1.backward()\n",
    "loss2.backward()\n",
    "print(\"The gradident of x2 from multiple backward accumulated is {}\".format(x2.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradident of x2 from one backpropagation but with summation is tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.FloatTensor([2])\n",
    "x1.requires_grad = True\n",
    "x2 = torch.FloatTensor([3])\n",
    "x2.requires_grad = True\n",
    "loss1 = x1*x2 - torch.sin(x2)\n",
    "loss2 = x1*x2 + torch.sin(x2)\n",
    "loss = loss1 + loss2\n",
    "loss.backward()\n",
    "print(\"The gradident of x2 from one backpropagation but with summation is {}\".format(x2.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can backpropagate twice with retain_graph=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradident of x2 from one backpropagation but with summation is tensor([8.])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.FloatTensor([2])\n",
    "x1.requires_grad = True\n",
    "x2 = torch.FloatTensor([3])\n",
    "x2.requires_grad = True\n",
    "loss1 = x1*x2 - torch.sin(x2)\n",
    "loss2 = x1*x2 + torch.sin(x2)\n",
    "loss = loss1 + loss2\n",
    "loss.backward(retain_graph=True)\n",
    "loss.backward()\n",
    "print(\"The gradident of x2 from one backpropagation but with summation is {}\".format(x2.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradident of x2 from one backpropagation but with summation and weights is tensor([3.4950])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.FloatTensor([2])\n",
    "x1.requires_grad = True\n",
    "x2 = torch.FloatTensor([3])\n",
    "x2.requires_grad = True\n",
    "loss1 = x1*x2 - torch.sin(x2)\n",
    "loss2 = x1*x2 + torch.sin(x2)\n",
    "loss = loss1 + loss2*0.5\n",
    "loss.backward()\n",
    "print(\"The gradident of x2 from one backpropagation but with summation and weights is {}\".format(x2.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pnm-new')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "769950f749cf263e04e34dc3f2141dd64b68c6436fc98dadab253467171bd3e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
